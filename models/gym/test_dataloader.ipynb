{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        # create a list of image/depth/lidar paths\n",
    "        self.data_dir = data_dir\n",
    "        self.image_paths = []\n",
    "        self.depth_paths = []\n",
    "        self.lidar_paths = []\n",
    "        self.image_paths, self.depth_paths, self.lidar_paths = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        # get folders\n",
    "        data_dir = self.data_dir\n",
    "        for folder in os.listdir(data_dir):\n",
    "            trajectory_folder_path = os.path.join(data_dir, folder)\n",
    "            if not os.path.isdir(trajectory_folder_path):\n",
    "                continue\n",
    "            print(f'Processing folder: {trajectory_folder_path}')\n",
    "\n",
    "            self.image_folder_name = 'image_lcam_fish'\n",
    "            self.depth_folder_name = 'depth_lcam_fish'\n",
    "            self.lidar_folder_name = 'lidar'\n",
    "            \n",
    "            image_folder_path = os.path.join(trajectory_folder_path, self.image_folder_name)\n",
    "            depth_folder_path = os.path.join(trajectory_folder_path, self.depth_folder_name)\n",
    "            lidar_folder_path = os.path.join(trajectory_folder_path, self.lidar_folder_name)\n",
    "\n",
    "            if not os.path.exists(image_folder_path):\n",
    "                continue\n",
    "            if not os.path.exists(depth_folder_path):\n",
    "                continue\n",
    "            if not os.path.exists(lidar_folder_path):\n",
    "                continue\n",
    "\n",
    "            # get image/depth/lidar paths\n",
    "            if len(os.listdir(image_folder_path)) != len(os.listdir(depth_folder_path)) \\\n",
    "                or len(os.listdir(image_folder_path)) != len(os.listdir(lidar_folder_path)) \\\n",
    "                or len(os.listdir(depth_folder_path)) != len(os.listdir(lidar_folder_path)):\n",
    "                print(f'Number of images, depth, and lidar files do not match in folder: {trajectory_folder_path}')\n",
    "                continue\n",
    "            self.image_paths += [os.path.join(image_folder_path, path) for path in os.listdir(image_folder_path)]\n",
    "            self.depth_paths += [os.path.join(depth_folder_path, path) for path in os.listdir(depth_folder_path)]\n",
    "            self.lidar_paths += [os.path.join(lidar_folder_path, path) for path in os.listdir(lidar_folder_path)]\n",
    "        print(f'Number of images: {len(self.image_paths)}')\n",
    "        print(f'Number of depth: {len(self.depth_paths)}')\n",
    "        print(f'Number of lidar: {len(self.lidar_paths)}')\n",
    "        return self.image_paths, self.depth_paths, self.lidar_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # read the image from disk\n",
    "        image_path = self.image_paths[index]\n",
    "        image = process_image(image_path)\n",
    "\n",
    "        # read the depth from disk\n",
    "        depth_path = self.depth_paths[index]\n",
    "        depth = process_depth(depth_path)\n",
    "\n",
    "        # read the lidar from disk\n",
    "        lidar_path = self.lidar_paths[index]\n",
    "        lidar = process_lidar(lidar_path)\n",
    "\n",
    "        return image, depth, lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform_image = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform_image(image)\n",
    "    return image\n",
    "image_path = \"/home/tyz/Desktop/11_777/Data_easy/P000/image_lcam_fish/000000_lcam_fish_image.png\"\n",
    "image = process_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "depth_path = '/home/tyz/Desktop/11_777/Data_easy/P000/depth_lcam_fish/000000_lcam_fish_depth.png'\n",
    "# print(depth.size)\n",
    "def process_depth(depth_path):\n",
    "    depth = Image.open(depth_path)\n",
    "    transform_depth = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5/2])\n",
    "    ])\n",
    "    depth = transform_depth(depth)\n",
    "    return depth[1:]\n",
    "depth = process_depth(depth_path)\n",
    "# print(depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def process_lidar(filename):\n",
    "  # Load point cloud data from file\n",
    "  pcd = o3d.io.read_point_cloud(filename)\n",
    "  points = np.asarray(pcd.points)\n",
    "\n",
    "  # Set voxel size\n",
    "  voxel_size = 0.1\n",
    "\n",
    "  # Voxelization\n",
    "  voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd,voxel_size=voxel_size)\n",
    "  voxels = np.asarray(voxel_grid.get_voxels())\n",
    "  # print(voxels.shape)\n",
    "\n",
    "  # Extract voxel features\n",
    "  features = []\n",
    "  for voxel in voxels:\n",
    "      voxel_indices = voxel.grid_index\n",
    "      if len(voxel_indices) == 0:\n",
    "          feature = np.zeros(6, dtype=np.float32)\n",
    "      else:\n",
    "          voxel_points = points[voxel_indices]\n",
    "          feature = np.concatenate([np.mean(voxel_points[:, :3], axis=0), np.max(voxel_points[:, :3], axis=0)])\n",
    "      features.append(feature)\n",
    "  features = np.stack(features)\n",
    "\n",
    "  # Normalize features\n",
    "  features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "\n",
    "  # Convert features to tensor\n",
    "  tensor = torch.from_numpy(features)\n",
    "  tensor = tensor.permute(1, 0).reshape(-1)  # (batch_size=1, num_channels=6, height=num_voxels, width=1)\n",
    "  padding=3*224*224-tensor.shape[-1]\n",
    "  tensor = torch.nn.functional.pad(tensor, (0, padding), mode='constant', value=0).reshape((3,224,224)).float()\n",
    "  # print(tensor)\n",
    "#   print(tensor.shape)\n",
    "  return tensor\n",
    "\n",
    "lidar=process_lidar('/home/tyz/Desktop/11_777/Data_easy/P000/lidar/000000_lcam_front_lidar.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.mask_token', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.key.weight']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_norm.weight', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder2.mask_token', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder1.decoder_embed.weight', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder1.decoder_pos_embed', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_embed.bias', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_embed.bias', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder1.decoder_layers.1.attention.attention.key.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_pred.bias', 'decoder1.mask_token', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder1.decoder_norm.bias', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_pred.weight', 'decoder1.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_embed.weight', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_norm.weight', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_norm.bias', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_pos_embed', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_pred.bias', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_pred.weight', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from multimodal.custom_models.CustomViT import CustomViT\n",
    "from multimodal.custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch.utils.data\n",
    "# call CustomViT\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "custom_model = custom_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P003\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P000\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P002\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P006\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P007\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P001\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P005\n",
      "Processing folder: /home/tyz/Desktop/11_777/Data_easy/P004\n",
      "Number of images: 9011\n",
      "Number of depth: 9011\n",
      "Number of lidar: 9011\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "# specify the directory containing the images\n",
    "data_dir = \"/home/tyz/Desktop/11_777/Data_easy\"\n",
    "# create the dataloader\n",
    "myDataset = MultimodalDataset(data_dir)\n",
    "dataloader = DataLoader(myDataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 1: loss 4.905\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 2: loss 4.804\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 3: loss 5.039\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 4: loss 4.084\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 5: loss 4.190\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 6: loss 3.656\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 7: loss 3.477\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 8: loss 2.995\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 9: loss 2.727\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 10: loss 3.298\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 11: loss 3.445\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 12: loss 4.401\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 13: loss 2.995\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 14: loss 2.860\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 15: loss 2.650\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 16: loss 2.604\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 17: loss 2.516\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 18: loss 2.447\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 19: loss 2.357\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 20: loss 2.277\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 21: loss 2.277\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 22: loss 2.274\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 23: loss 2.260\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 24: loss 2.184\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 25: loss 2.215\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 26: loss 2.164\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 27: loss 2.220\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 28: loss 2.144\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 29: loss 2.164\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 30: loss 2.119\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 31: loss 2.128\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 32: loss 2.146\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 33: loss 2.072\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 34: loss 2.091\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 35: loss 2.088\n",
      "torch.Size([64, 3, 224, 224])\n",
      "Epoch 1, Batch 36: loss 2.020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m     batch_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mfor\u001b[39;00m image_batch, depth_batch, lidar_batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      7\u001b[0m         \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m         \u001b[39mprint\u001b[39m(image_batch\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1296\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(custom_model.parameters(), lr=0.001)\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = 0\n",
    "    for image_batch, depth_batch, lidar_batch in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        print(image_batch.shape)\n",
    "        # Forward pass\n",
    "        image_batch = image_batch.cuda()\n",
    "        depth_batch = depth_batch.cuda()\n",
    "        lidar_batch = lidar_batch.cuda()\n",
    "        outputs = custom_model(image_batch,depth_batch,lidar_batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        # running_loss += loss.item()\n",
    "        # if epoch % 10 == 9:\n",
    "        print(f'Epoch {epoch + 1}, Batch {batch_count + 1}: loss {loss / 1:.3f}')\n",
    "        # running_loss = 0.0\n",
    "        batch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'mask', 'ids_restore', 'hidden_states'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs.hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image device cuda:0\n",
      "model device cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"image device\",image_batch.device)\n",
    "print(\"model device\",next(custom_model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(custom_model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_pred.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.mask_token', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.key.weight']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_norm.weight', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder2.mask_token', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder1.decoder_embed.weight', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder1.decoder_pos_embed', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_embed.bias', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_embed.bias', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder1.decoder_layers.1.attention.attention.key.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_pred.bias', 'decoder1.mask_token', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder1.decoder_norm.bias', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_pred.weight', 'decoder1.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_embed.weight', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_norm.weight', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_norm.bias', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_pos_embed', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_pred.bias', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_pred.weight', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTMAEForPreTraining, ViTMAEConfig\n",
    "from multimodal.custom_models.CustomViT import CustomViT\n",
    "from multimodal.custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch.utils.data\n",
    "# call CustomViT\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "custom_model = custom_model.cuda()\n",
    "custom_model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
