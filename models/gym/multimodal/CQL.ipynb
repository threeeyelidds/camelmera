{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myadix\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "from transformers import ViTMAEConfig\n",
    "\n",
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "from transformers.models.vit_mae.modeling_vit_mae import ViTMAEModel\n",
    "# from tem_dataloader import MultimodalDatasetPerTrajectory\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from d3rlpy.algos import CQL\n",
    "from d3rlpy.dataset import Episode, MDPDataset, Transition\n",
    "wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.mask_token', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder2.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_pos_embed', 'decoder1.decoder_embed.weight', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_norm.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder2.decoder_pred.bias', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder2.mask_token', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_norm.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder1.mask_token', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_pred.weight', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder1.decoder_pred.bias', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_embed.weight', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder1.decoder_layers.1.attention.attention.key.weight', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder1.decoder_pos_embed', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder1.decoder_embed.bias', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_norm.bias', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_norm.weight', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_pred.weight', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_embed.bias', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.4.attention.attention.key.weight', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.6.attention.attention.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch\n",
    "# call CustomViT\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from PIL import Image\n",
    "\n",
    "# output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "trained_model_name = 'multimodal'\n",
    "output_dir='/home/ubuntu/weights/' + trained_model_name\n",
    "\n",
    "# Initialize a new CustomViTMAE model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "\n",
    "# Load the state_dict from the saved model\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Apply the state_dict to the custom_model\n",
    "custom_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Processing folder: /mnt/data/tartanairv2filtered/AbandonedCableExposure/Data_easy/P001\n",
      "Number of images: 1183\n",
      "Number of depth: 1183\n",
      "Number of lidar: 1183\n",
      "Number of pose: 1183\n"
     ]
    }
   ],
   "source": [
    "from tem_dataloader import MultimodalDatasetPerTrajectory\n",
    "# environment_name = 'AmericanDinerExposure'\n",
    "# environemnt_directory = f'/media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/{environment_name}/Data_easy'\n",
    "# my_dataset = MultimodalDatasetPerTrajectory(environemnt_directory)\n",
    "environment_name = 'AbandonedCableExposure'\n",
    "environemnt_directory = f'/mnt/data/tartanairv2filtered/{environment_name}/Data_easy'\n",
    "OBSERVATION_SIZE = 768\n",
    "ACTION_SIZE = 7\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for folder in os.listdir(environemnt_directory):\n",
    "    trajectory_folder_path = os.path.join(environemnt_directory, folder)\n",
    "    if not os.path.isdir(trajectory_folder_path):\n",
    "        continue\n",
    "    my_dataset = MultimodalDatasetPerTrajectory(trajectory_folder_path)\n",
    "    train_dataloader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reward_function(state_embedding, goal_embedding, threshold=0.05, goal_reward=100):\n",
    "    distance = np.linalg.norm(state_embedding - goal_embedding)\n",
    "\n",
    "    if distance <= threshold:\n",
    "        # Give a large positive reward when the goal is reached\n",
    "        reward = goal_reward\n",
    "    else:\n",
    "        # Give a negative reward proportional to the distance otherwise\n",
    "        reward = -distance\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 14.61 GiB total capacity; 10.91 GiB already allocated; 129.19 MiB free; 11.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m pixel_values1 \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mpixel_values1\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     17\u001b[0m pixel_values2 \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mpixel_values2\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m custom_model(pixel_values,pixel_values1,pixel_values2,noise\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     19\u001b[0m embedding \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mhidden_states[:,\u001b[39m0\u001b[39m,:]\n\u001b[1;32m     20\u001b[0m observation \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/camelmera/models/gym/multimodal/custom_models/CustomViTMAE.py:51\u001b[0m, in \u001b[0;36mCustomViTMAE.forward\u001b[0;34m(self, pixel_values, pixel_values1, pixel_values2, noise, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m logits \u001b[39m=\u001b[39m decoder_outputs\u001b[39m.\u001b[39mlogits  \u001b[39m# shape (batch_size, num_patches, patch_size*patch_size*num_channels)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m combined_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_loss(pixel_values, logits, mask)\n\u001b[0;32m---> 51\u001b[0m decoder_outputs1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder1(latent, ids_restore1)\n\u001b[1;32m     52\u001b[0m logits1 \u001b[39m=\u001b[39m decoder_outputs1\u001b[39m.\u001b[39mlogits  \u001b[39m# shape (batch_size, num_patches, patch_size*patch_size*num_channels)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m combined_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_loss(pixel_values1, logits1, mask1)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:809\u001b[0m, in \u001b[0;36mViTMAEDecoder.forward\u001b[0;34m(self, hidden_states, ids_restore, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    803\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    804\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    805\u001b[0m         hidden_states,\n\u001b[1;32m    806\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    808\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 809\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, head_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, output_attentions\u001b[39m=\u001b[39;49moutput_attentions)\n\u001b[1;32m    811\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:490\u001b[0m, in \u001b[0;36mViTMAELayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    487\u001b[0m     head_mask: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    488\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    489\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Tuple[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> 490\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    491\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayernorm_before(hidden_states),  \u001b[39m# in ViTMAE, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[1;32m    492\u001b[0m         head_mask,\n\u001b[1;32m    493\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    494\u001b[0m     )\n\u001b[1;32m    495\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    496\u001b[0m     outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:429\u001b[0m, in \u001b[0;36mViTMAEAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    424\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    425\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    426\u001b[0m     head_mask: Optional[torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    427\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    428\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Tuple[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> 429\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden_states, head_mask, output_attentions)\n\u001b[1;32m    431\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    433\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/transformers/models/vit_mae/modeling_vit_mae.py:352\u001b[0m, in \u001b[0;36mViTMAESelfAttention.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    349\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    351\u001b[0m \u001b[39m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m attention_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query_layer, key_layer\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[1;32m    354\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    356\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 152.00 MiB (GPU 0; 14.61 GiB total capacity; 10.91 GiB already allocated; 129.19 MiB free; 11.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize empty arrays for observations, actions, rewards, and terminals\n",
    "all_observations = np.empty((0, OBSERVATION_SIZE))\n",
    "all_actions = np.empty((0, ACTION_SIZE))\n",
    "all_rewards = np.empty(0)\n",
    "all_terminals = np.empty(0, dtype=bool)\n",
    "\n",
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    # get embedding\n",
    "    custom_model.cuda()\n",
    "    custom_model.eval()\n",
    "    pixel_values = data[\"pixel_values\"].cuda()\n",
    "    pixel_values1 = data[\"pixel_values1\"].cuda()\n",
    "    pixel_values2 = data[\"pixel_values2\"].cuda()\n",
    "    outputs = custom_model(pixel_values,pixel_values1,pixel_values2,noise=None)\n",
    "    embedding = outputs.hidden_states[:,0,:]\n",
    "    observation = embedding.cpu().detach().numpy()\n",
    "    # get action\n",
    "    pose = data[\"pose_values\"]\n",
    "    action = torch.diff(pose,axis = 0).numpy()\n",
    "    action = np.concatenate((action, np.zeros((1,7))), axis=0)\n",
    "    # get reward\n",
    "    goal = observation[-1]\n",
    "    partial_function = functools.partial(reward_function, goal_embedding=goal)\n",
    "    reward = np.apply_along_axis(partial_function, 1, observation)\n",
    "    # get terminals\n",
    "    terminals = [False]*data.shape[0]\n",
    "    terminals[-1]=True\n",
    "    terminals = np.array(terminals)\n",
    "\n",
    "    # Concatenate observations, actions, rewards, and terminals\n",
    "    all_observations = np.vstack((all_observations, observation))\n",
    "    all_actions = np.vstack((all_actions, action))\n",
    "    all_rewards = np.hstack((all_rewards, reward))\n",
    "    all_terminals = np.hstack((all_terminals, terminals))\n",
    "\n",
    "print(\"All observations shape:\", all_observations.shape)\n",
    "print(\"All actions shape:\", all_actions.shape)\n",
    "print(\"All rewards shape:\", all_rewards.shape)\n",
    "print(\"All terminals shape:\", all_terminals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Args:\n",
    "        observations (numpy.ndarray): N-D array. If the\n",
    "            observation is a vector, the shape should be\n",
    "            `(N, dim_observation)`. If the observations is an image, the shape\n",
    "            should be `(N, C, H, W)`.\n",
    "        actions (numpy.ndarray): N-D array. If the actions-space is\n",
    "            continuous, the shape should be `(N, dim_action)`. If the\n",
    "            action-space is discrete, the shape should be `(N,)`.\n",
    "        rewards (numpy.ndarray): array of scalar rewards. The reward function\n",
    "            should be defined as :math:`r_t = r(s_t, a_t)`.\n",
    "        terminals (numpy.ndarray): array of binary terminal flags.\n",
    "        episode_terminals (numpy.ndarray): array of binary episode terminal\n",
    "            flags. The given data will be splitted based on this flag.\n",
    "            This is useful if you want to specify the non-environment\n",
    "            terminations (e.g. timeout). If ``None``, the episode terminations\n",
    "            match the environment terminations.\n",
    "        discrete_action (bool): flag to use the given actions as discrete\n",
    "            action-space actions. If ``None``, the action type is automatically\n",
    "            determined.\n",
    "    '''\n",
    "cql_dataset = MDPDataset(observations=observation,\\\n",
    "                         actions=action,\\\n",
    "                         rewards=np.zeros(32),\\\n",
    "                         terminals=np.zeros(32),\\\n",
    "                         episode_terminals=np.array([False]*embedding.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import CQL\n",
    "\n",
    "# setup CQL algorithm\n",
    "cql = CQL(use_gpu=False)\n",
    "\n",
    "# split train and test episodes\n",
    "# train_episodes, test_episodes = train_test_split(cql_dataset, test_size=0.25)\n",
    "\n",
    "# start training\n",
    "cql.fit(cql_dataset,\n",
    "        eval_episodes=None,\n",
    "        n_epochs=1,\n",
    "        scorers=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
