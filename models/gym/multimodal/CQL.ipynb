{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch\n",
    "# call CustomViT\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from PIL import Image\n",
    "\n",
    "output_dir='/home/ubuntu/camelmera'\n",
    "# trained_model_name = 'multimodal'\n",
    "# output_dir='/home/ubuntu/weights/' + trained_model_name\n",
    "\n",
    "# Initialize a new CustomViT model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT(config=vit_config)\n",
    "\n",
    "# Initialize a new CustomViTMAE model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "custom_model = CustomViTMAE(config=config)\n",
    "custom_model.vit = vit_model\n",
    "\n",
    "# Load the state_dict from the saved model\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "custom_model.load_state_dict(state_dict)\n",
    "\n",
    "# don't need decoders\n",
    "vit_encoder = custom_model.vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reward_function(state_embedding, goal_embedding, threshold=0.02, goal_reward=100):\n",
    "    distance = np.linalg.norm(state_embedding - goal_embedding)\n",
    "\n",
    "    if distance <= threshold:\n",
    "        # Give a large positive reward when the goal is reached\n",
    "        reward = goal_reward\n",
    "    else:\n",
    "        # Give a negative reward proportional to the distance otherwise\n",
    "        reward = -distance\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tem_dataloader import MultimodalDatasetPerTrajectory\n",
    "import functools\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "environment_name = 'AbandonedFactoryExposure'\n",
    "environemnt_directory = f'/mnt/temp_mount/{environment_name}/Data_hard'\n",
    "OBSERVATION_SIZE = 768\n",
    "ACTION_SIZE = 7\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "for i in range(0,10):\n",
    "    if i==7:\n",
    "        continue\n",
    "    trajectory_folder_path = os.path.join(environemnt_directory, f'P00{i}')\n",
    "    my_dataset = MultimodalDatasetPerTrajectory(trajectory_folder_path)\n",
    "    train_dataloader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize empty arrays for observations, actions, rewards, and terminals\n",
    "    all_observations = np.empty((0, OBSERVATION_SIZE))\n",
    "    all_actions = np.empty((0, ACTION_SIZE))\n",
    "    all_rewards = np.empty(0)\n",
    "    all_terminals = np.empty(0, dtype=bool)\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        # get embedding\n",
    "        vit_encoder.cuda()\n",
    "        vit_encoder.eval()\n",
    "        pixel_values = data[\"pixel_values\"].cuda()\n",
    "        pixel_values1 = data[\"pixel_values1\"].cuda()\n",
    "        pixel_values2 = data[\"pixel_values2\"].cuda()\n",
    "        outputs = vit_encoder(pixel_values,pixel_values1,pixel_values2,noise=None)\n",
    "        embedding = outputs.last_hidden_state[:,0,:]\n",
    "        observation = embedding.cpu().detach().numpy()\n",
    "        # get action\n",
    "        pose = data[\"pose_values\"]\n",
    "        action = torch.diff(pose,axis = 0).numpy()\n",
    "        action = np.concatenate((action, np.zeros((1,7))), axis=0)\n",
    "        # get reward\n",
    "        goal = observation[-1]\n",
    "        partial_function = functools.partial(reward_function, goal_embedding=goal)\n",
    "        reward = np.apply_along_axis(partial_function, 1, observation)\n",
    "        # get terminals\n",
    "        terminals = np.zeros_like(reward, dtype=int)\n",
    "        terminals[reward == 100] = 1\n",
    "\n",
    "        # Concatenate observations, actions, rewards, and terminals\n",
    "        all_observations = np.vstack((all_observations, observation))\n",
    "        all_actions = np.vstack((all_actions, action))\n",
    "        all_rewards = np.hstack((all_rewards, reward))\n",
    "        all_terminals = np.hstack((all_terminals, terminals))\n",
    "\n",
    "    print(\"All observations shape:\", all_observations.shape)\n",
    "    print(\"All actions shape:\", all_actions.shape)\n",
    "    print(\"All rewards shape:\", all_rewards.shape)\n",
    "    print(\"All terminals shape:\", all_terminals.shape)\n",
    "    \n",
    "    np.save(f'hard/all_observations_P00{i}.npy', all_observations)\n",
    "    np.save(f'hard/all_actions_P00{i}.npy', all_actions)\n",
    "    np.save(f'hard/all_rewards_P00{i}.npy', all_rewards)\n",
    "    np.save(f'hard/all_terminals_P00{i}.npy', all_terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Args:\n",
    "        observations (numpy.ndarray): N-D array. If the\n",
    "            observation is a vector, the shape should be\n",
    "            `(N, dim_observation)`. If the observations is an image, the shape\n",
    "            should be `(N, C, H, W)`.\n",
    "        actions (numpy.ndarray): N-D array. If the actions-space is\n",
    "            continuous, the shape should be `(N, dim_action)`. If the\n",
    "            action-space is discrete, the shape should be `(N,)`.\n",
    "        rewards (numpy.ndarray): array of scalar rewards. The reward function\n",
    "            should be defined as :math:`r_t = r(s_t, a_t)`.\n",
    "        terminals (numpy.ndarray): array of binary terminal flags.\n",
    "        episode_terminals (numpy.ndarray): array of binary episode terminal\n",
    "            flags. The given data will be splitted based on this flag.\n",
    "            This is useful if you want to specify the non-environment\n",
    "            terminations (e.g. timeout). If ``None``, the episode terminations\n",
    "            match the environment terminations.\n",
    "        discrete_action (bool): flag to use the given actions as discrete\n",
    "            action-space actions. If ``None``, the action type is automatically\n",
    "            determined.\n",
    "    '''\n",
    "hard_all_observations = np.load('hard/all_observations.npy')\n",
    "hard_all_actions = np.load('hard/all_actions.npy')\n",
    "hard_all_rewards = np.load('hard/all_rewards.npy')\n",
    "hard_all_terminals = np.load('hard/all_terminals.npy')\n",
    "print(np.count_nonzero(hard_all_terminals == 1))\n",
    "# cql_dataset = MDPDataset(observations=all_observations,actions=all_actions,rewards=all_rewards,terminals=all_terminals,episode_terminals=all_terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_observations = np.load('all_observations.npy')\n",
    "all_actions = np.load('all_actions.npy')\n",
    "all_rewards = np.load('all_rewards.npy')\n",
    "all_terminals = np.load('all_terminals.npy')\n",
    "print(np.count_nonzero(all_terminals == 1))\n",
    "# all_observations = np.vstack((all_observations, hard_all_observations))\n",
    "# all_actions = np.vstack((all_actions, hard_all_actions))\n",
    "# all_rewards = np.hstack((all_rewards, hard_all_rewards))\n",
    "# all_terminals = np.hstack((all_terminals, hard_all_terminals))\n",
    "# print(\"All observations shape:\", all_observations.shape)\n",
    "# print(\"All actions shape:\", all_actions.shape)\n",
    "# print(\"All rewards shape:\", all_rewards.shape)\n",
    "# print(\"All terminals shape:\", all_terminals.shape)\n",
    "cql_dataset = MDPDataset(observations=all_observations,actions=all_actions,rewards=all_rewards,terminals=all_terminals,episode_terminals=all_terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cql_dataset.actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import CQL\n",
    "\n",
    "# setup CQL algorithm\n",
    "cql = CQL(use_gpu=True, initial_alpha=1.0)\n",
    "\n",
    "# split train and test episodes\n",
    "# train_episodes, test_episodes = train_test_split(cql_dataset, test_size=0.25)\n",
    "\n",
    "# start training\n",
    "cql.fit(cql_dataset,\n",
    "        eval_episodes=None,\n",
    "        n_epochs=80,\n",
    "        scorers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(all_rewards[0:64]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from d3rlpy.dataset import Episode, MDPDataset, Transition\n",
    "\n",
    "OBSERVATION_SIZE = 768\n",
    "ACTION_SIZE = 7\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "all_observations = np.empty((0, OBSERVATION_SIZE))\n",
    "all_actions = np.empty((0, ACTION_SIZE))\n",
    "all_rewards = np.empty(0)\n",
    "all_terminals = np.empty(0, dtype=bool)\n",
    "\n",
    "for i in range(0,10):\n",
    "    if i==7:\n",
    "        continue\n",
    "    observation = np.load(f'hard/all_observations_P00{i}.npy')\n",
    "    action = np.load(f'hard/all_actions_P00{i}.npy')\n",
    "    reward = np.load(f'hard/all_rewards_P00{i}.npy')\n",
    "    terminals = np.load(f'hard/all_terminals_P00{i}.npy')\n",
    "\n",
    "    all_observations = np.vstack((all_observations, observation))\n",
    "    all_actions = np.vstack((all_actions, action))\n",
    "    all_rewards = np.hstack((all_rewards, reward))\n",
    "    all_terminals = np.hstack((all_terminals, terminals))\n",
    "\n",
    "    print(\"All observations shape:\", all_observations.shape)\n",
    "    print(\"All actions shape:\", all_actions.shape)\n",
    "    print(\"All rewards shape:\", all_rewards.shape)\n",
    "    print(\"All terminals shape:\", all_terminals.shape)\n",
    "cql_dataset = MDPDataset(observations=all_observations,actions=all_actions,rewards=all_rewards,terminals=all_terminals,episode_terminals=all_terminals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import CQL\n",
    "cql01 = CQL(use_gpu=False)\n",
    "cql01.build_with_dataset(cql_dataset)\n",
    "cql01.load_model('/home/ubuntu/camelmera/models/gym/multimodal/d3rlpy_logs/CQL_20230503011241/model_40.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d3rlpy.algos import CQL\n",
    "\n",
    "cql = CQL.from_json('d3rlpy_logs/CQL_20230504012746/params.json')\n",
    "\n",
    "# ready to load\n",
    "cql.load_model('d3rlpy_logs/CQL_20230504012746/model_310.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "cql.fit(cql_dataset,\n",
    "        eval_episodes=None,\n",
    "        n_epochs=30,\n",
    "        scorers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "cql.fit(cql_dataset,\n",
    "        eval_episodes=None,\n",
    "        n_epochs=10,\n",
    "        scorers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_rewards = np.load('all_rewards.npy')\n",
    "print(all_rewards[0:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "all_observations = np.load('all_observations.npy')\n",
    "# observation = np.load('hard/all_observations_P000.npy')\n",
    "print(all_observations.shape)\n",
    "# get reward\n",
    "all_rewards = np.empty(0)\n",
    "all_terminals = np.empty(0)\n",
    "for i in range(0,len(all_observations),64): \n",
    "    goal = None\n",
    "    if i+63 < len(all_observations):\n",
    "        goal = all_observations[i+63]\n",
    "        observation = all_observations[i:i+64]\n",
    "    else:\n",
    "        goal = all_observations[-1]\n",
    "        observation = all_observations[i:]\n",
    "    partial_function = functools.partial(reward_function, goal_embedding=goal,threshold=0.01)\n",
    "    reward = np.apply_along_axis(partial_function, 1, observation)\n",
    "    all_rewards = np.hstack((all_rewards, reward))\n",
    "    # get terminals\n",
    "    terminal = np.zeros_like(reward, dtype=int)\n",
    "    terminal[reward == 100] = 1\n",
    "    # print(np.count_nonzero(terminal == 1))\n",
    "    all_terminals = np.hstack((all_terminals,terminal))\n",
    "print(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_100 = np.count_nonzero(all_terminals == 1)\n",
    "print(\"Number of terminals:\", count_100)\n",
    "print(len(all_terminals)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all_terminals.npy',all_terminals)\n",
    "np.save('all_rewards.npy',all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_terminals = np.load('hard/all_terminals_P000.npy')\n",
    "print(original_terminals)\n",
    "print(np.count_nonzero(original_terminals == 1))\n",
    "print(len(original_terminals)/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(reward[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check difference around goal = observation[-100]\n",
    "downsampled_observation = observation[::10,:]\n",
    "print(downsampled_observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get reward\n",
    "goal = downsampled_observation[-23]\n",
    "partial_function = functools.partial(reward_function, goal_embedding=goal,threshold=0.02)\n",
    "reward = np.apply_along_axis(partial_function, 1, downsampled_observation)\n",
    "# get terminals\n",
    "terminals = np.zeros_like(reward, dtype=int)\n",
    "terminals[reward == 100] = 1\n",
    "count_100 = np.count_nonzero(reward == 100)\n",
    "print(\"Number of elements with the value 100:\", count_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_actions = np.load('all_actions.npy')\n",
    "hard_actions = np.load('hard/all_actions.npy')\n",
    "print(np.min(easy_actions,axis=0)[0],np.max(easy_actions,axis=0)[0],np.mean(easy_actions,axis=0)[0],np.std(easy_actions,axis=0)[0])\n",
    "print(np.min(easy_actions,axis=0)[3],np.max(easy_actions,axis=0)[3],np.mean(easy_actions,axis=0)[3],np.std(easy_actions,axis=0)[3])\n",
    "print(np.min(hard_actions,axis=0)[0],np.max(hard_actions,axis=0)[0],np.mean(hard_actions,axis=0)[0],np.std(hard_actions,axis=0)[0])\n",
    "print(np.min(hard_actions,axis=0)[3],np.max(hard_actions,axis=0)[3],np.mean(hard_actions,axis=0)[3],np.std(hard_actions,axis=0)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
