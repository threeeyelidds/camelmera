{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myadix\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import wandb\n",
    "from transformers import ViTMAEConfig\n",
    "\n",
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "from transformers.models.vit_mae.modeling_vit_mae import ViTMAEModel\n",
    "# from tem_dataloader import MultimodalDatasetPerTrajectory\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from d3rlpy.algos import CQL\n",
    "from d3rlpy.dataset import Episode, MDPDataset, Transition\n",
    "wandb.login() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_pos_embed', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.mask_token', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.bias']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_norm.bias', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder2.decoder_layers.1.layernorm_after.weight', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_pred.bias', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_norm.weight', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_embed.bias', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_layers.4.attention.attention.key.weight', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder1.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder1.mask_token', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_pred.bias', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder2.decoder_embed.bias', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_norm.weight', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder2.decoder_embed.weight', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder1.decoder_norm.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder1.decoder_embed.weight', 'decoder1.decoder_pos_embed', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder2.mask_token', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder2.decoder_pred.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder2.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_pos_embed', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_pred.weight', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch\n",
    "# call CustomViT\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from PIL import Image\n",
    "\n",
    "output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "\n",
    "# Initialize a new CustomViTMAE model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "\n",
    "# Load the state_dict from the saved model\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Apply the state_dict to the custom_model\n",
    "custom_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P000\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P001\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P002\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P003\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P004\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P005\n",
      "Number of images: 1819\n",
      "Number of depth: 1819\n",
      "Number of lidar: 1819\n"
     ]
    }
   ],
   "source": [
    "from tem_dataloader import MultimodalDatasetPerTrajectory\n",
    "environment_name = 'AmericanDinerExposure'\n",
    "environemnt_directory = f'/media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/{environment_name}/Data_easy'\n",
    "my_dataset = MultimodalDatasetPerTrajectory(environemnt_directory)\n",
    "for folder in os.listdir(environemnt_directory):\n",
    "    trajectory_folder_path = os.path.join(environemnt_directory, folder)\n",
    "    if not os.path.isdir(trajectory_folder_path):\n",
    "        continue\n",
    "    my_dataset = MultimodalDatasetPerTrajectory(trajectory_folder_path)\n",
    "    train_dataloader = DataLoader(my_dataset, batch_size=32, shuffle=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(my_dataset, batch_size=batch_size, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    custom_model.cuda()\n",
    "    custom_model.eval()\n",
    "    pixel_values = data[\"pixel_values\"].cuda()\n",
    "    pixel_values1 = data[\"pixel_values1\"].cuda()\n",
    "    pixel_values2 = data[\"pixel_values2\"].cuda()\n",
    "    outputs = custom_model(pixel_values,pixel_values1,pixel_values2,noise=None)\n",
    "    embedding = outputs.hidden_states[:,0,:]\n",
    "    print(embedding.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(trajectory_folder_path):\n",
    "    pose = []\n",
    "    pose_file_path = os.path.join(\n",
    "            trajectory_folder_path, 'pose_lcam_front.txt')\n",
    "    with open(pose_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose_list = line.split(\" \")\n",
    "            pose_list = [float(_) for _ in pose_list]\n",
    "            pose.append(torch.Tensor(pose_list))\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql_dataset = MDPDataset(observations=consolidated_dataset['observations'],\\\n",
    "                         actions=consolidated_dataset['actions'],\\\n",
    "                         rewards=consolidated_dataset['rewards'],\\\n",
    "                         terminal_flags=[False]*len(consolidated_dataset['observations']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
