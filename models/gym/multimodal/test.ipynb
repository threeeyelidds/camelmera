{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myadix\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() \n",
    "# 8599fbb702cb5767e13d2ac3b1cdcc1c9b65d451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P001\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P002\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P003\n",
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: /media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/AmericanDinerExposure/Data_easy/P005\n",
      "Number of images: 1819\n",
      "Number of depth: 1819\n",
      "Number of lidar: 1819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "from tem_dataloader import MultimodalDataset\n",
    "environment_name = 'AmericanDinerExposure'\n",
    "\n",
    "my_dataset = MultimodalDataset(f'/media/tyz/3B6FFE7354FF3296/11_777/tartanairv2filtered/{environment_name}/Data_easy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(my_dataset, batch_size=batch_size, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.query.bias']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder1.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder1.decoder_pred.bias', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_embed.bias', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder2.decoder_layers.6.attention.attention.value.bias', 'decoder1.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_norm.weight', 'decoder2.mask_token', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_pos_embed', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_embed.weight', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder1.mask_token', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_norm.weight', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder2.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder1.decoder_pred.weight', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder2.decoder_pred.bias', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder1.decoder_norm.bias', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_norm.bias', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_pos_embed', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_embed.bias', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_pred.weight', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_embed.weight', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.4.attention.attention.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch\n",
    "# call CustomViT\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from PIL import Image\n",
    "\n",
    "output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "\n",
    "# Initialize a new CustomViTMAE model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "\n",
    "# Load the state_dict from the saved model\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Apply the state_dict to the custom_model\n",
    "custom_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing CustomViT: ['decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.mask_token', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.0.intermediate.dense.bias', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_norm.bias', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.6.attention.attention.query.bias']\n",
      "- This IS expected if you are initializing CustomViT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomViT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomViTMAE were not initialized from the model checkpoint at facebook/vit-mae-base and are newly initialized: ['decoder1.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.0.attention.attention.value.bias', 'decoder1.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.5.output.dense.weight', 'decoder1.decoder_layers.1.layernorm_before.bias', 'decoder2.decoder_layers.6.attention.attention.key.weight', 'decoder2.decoder_layers.2.output.dense.bias', 'decoder1.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.0.layernorm_after.weight', 'decoder2.decoder_layers.3.layernorm_after.weight', 'decoder2.decoder_layers.6.layernorm_after.bias', 'decoder2.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.query.bias', 'decoder2.decoder_layers.7.attention.output.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.bias', 'decoder2.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.4.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.key.weight', 'decoder1.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.3.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.query.bias', 'decoder1.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.6.attention.attention.value.bias', 'decoder2.decoder_layers.7.attention.attention.value.weight', 'decoder1.decoder_layers.3.output.dense.bias', 'decoder1.decoder_layers.5.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.query.bias', 'decoder2.decoder_layers.3.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.0.layernorm_before.bias', 'decoder2.decoder_layers.3.output.dense.weight', 'decoder1.decoder_layers.2.attention.attention.query.weight', 'decoder1.decoder_layers.2.layernorm_before.weight', 'decoder1.decoder_layers.3.output.dense.weight', 'decoder2.decoder_layers.4.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.1.attention.attention.key.bias', 'decoder1.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.4.attention.attention.key.weight', 'decoder1.decoder_layers.2.layernorm_before.bias', 'decoder2.decoder_layers.4.layernorm_after.weight', 'decoder1.decoder_layers.6.attention.attention.key.weight', 'decoder1.decoder_layers.1.layernorm_after.bias', 'decoder1.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.4.attention.output.dense.weight', 'decoder1.decoder_pred.bias', 'decoder2.decoder_layers.1.intermediate.dense.bias', 'decoder1.decoder_layers.7.layernorm_before.bias', 'decoder2.decoder_layers.1.attention.attention.key.bias', 'decoder2.decoder_layers.6.output.dense.bias', 'decoder2.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.4.attention.attention.query.weight', 'decoder1.decoder_layers.5.attention.output.dense.weight', 'decoder2.decoder_embed.bias', 'decoder1.decoder_layers.0.attention.attention.query.weight', 'decoder2.decoder_layers.6.attention.attention.value.bias', 'decoder1.decoder_layers.1.attention.attention.key.weight', 'decoder1.decoder_layers.0.intermediate.dense.bias', 'decoder1.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_layers.2.layernorm_after.weight', 'decoder1.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.4.attention.attention.value.weight', 'decoder2.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.3.attention.attention.key.bias', 'decoder2.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.7.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.key.bias', 'decoder1.decoder_layers.4.layernorm_after.weight', 'decoder2.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.5.layernorm_after.bias', 'decoder2.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_layers.4.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.attention.value.bias', 'decoder1.decoder_layers.0.intermediate.dense.weight', 'decoder2.decoder_layers.2.layernorm_before.bias', 'decoder1.decoder_layers.0.attention.attention.key.weight', 'decoder2.decoder_layers.2.attention.attention.query.bias', 'decoder1.decoder_layers.0.output.dense.bias', 'decoder1.decoder_layers.6.layernorm_before.bias', 'decoder1.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.4.output.dense.weight', 'decoder1.decoder_layers.6.layernorm_after.weight', 'decoder2.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.2.attention.output.dense.weight', 'decoder2.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_norm.weight', 'decoder2.mask_token', 'decoder1.decoder_layers.5.intermediate.dense.bias', 'decoder1.decoder_layers.4.intermediate.dense.weight', 'decoder1.decoder_layers.5.attention.attention.key.weight', 'decoder1.decoder_layers.7.attention.output.dense.bias', 'decoder1.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.2.attention.attention.value.bias', 'decoder1.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_pos_embed', 'decoder1.decoder_layers.3.attention.output.dense.weight', 'decoder1.decoder_layers.4.attention.output.dense.weight', 'decoder2.decoder_embed.weight', 'decoder2.decoder_layers.5.layernorm_after.bias', 'decoder1.decoder_layers.3.attention.attention.value.weight', 'decoder2.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.1.output.dense.weight', 'decoder1.mask_token', 'decoder1.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.6.intermediate.dense.weight', 'decoder2.decoder_layers.0.attention.attention.value.bias', 'decoder2.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.key.bias', 'decoder1.decoder_layers.2.attention.output.dense.bias', 'decoder1.decoder_layers.6.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.attention.value.bias', 'decoder1.decoder_layers.6.attention.attention.value.weight', 'decoder1.decoder_layers.2.attention.attention.value.bias', 'decoder2.decoder_layers.3.layernorm_before.bias', 'decoder1.decoder_norm.weight', 'decoder1.decoder_layers.6.layernorm_after.bias', 'decoder1.decoder_layers.3.layernorm_after.bias', 'decoder1.decoder_layers.1.attention.attention.query.bias', 'decoder2.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.4.layernorm_after.bias', 'decoder1.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_layers.2.attention.attention.query.weight', 'decoder2.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.0.attention.output.dense.weight', 'decoder1.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.2.attention.output.dense.weight', 'decoder1.decoder_layers.7.attention.output.dense.weight', 'decoder1.decoder_pred.weight', 'decoder2.decoder_layers.6.attention.attention.key.bias', 'decoder2.decoder_layers.0.layernorm_after.bias', 'decoder2.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_layers.5.layernorm_before.bias', 'decoder1.decoder_layers.0.layernorm_after.bias', 'decoder1.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.0.output.dense.weight', 'decoder1.decoder_layers.7.layernorm_after.weight', 'decoder1.decoder_layers.4.attention.attention.value.bias', 'decoder1.decoder_layers.2.attention.attention.key.weight', 'decoder1.decoder_layers.2.attention.attention.key.bias', 'decoder2.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.3.attention.output.dense.weight', 'decoder2.decoder_layers.0.intermediate.dense.bias', 'decoder2.decoder_pred.bias', 'decoder2.decoder_layers.5.attention.attention.value.bias', 'decoder2.decoder_layers.0.attention.attention.query.weight', 'decoder1.decoder_layers.6.attention.output.dense.bias', 'decoder1.decoder_layers.3.attention.attention.query.weight', 'decoder2.decoder_layers.5.output.dense.weight', 'decoder2.decoder_layers.1.layernorm_before.bias', 'decoder2.decoder_layers.2.attention.output.dense.bias', 'decoder2.decoder_layers.1.attention.attention.value.weight', 'decoder2.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.6.layernorm_after.weight', 'decoder1.decoder_norm.bias', 'decoder1.decoder_layers.1.attention.output.dense.weight', 'decoder2.decoder_layers.5.attention.output.dense.weight', 'decoder1.decoder_layers.5.attention.attention.query.weight', 'decoder2.decoder_layers.5.attention.attention.key.bias', 'decoder2.decoder_layers.7.attention.attention.key.bias', 'decoder1.decoder_layers.7.layernorm_before.weight', 'decoder2.decoder_norm.bias', 'decoder2.decoder_layers.5.attention.attention.query.weight', 'decoder1.decoder_layers.7.attention.attention.key.bias', 'decoder2.decoder_layers.4.intermediate.dense.weight', 'decoder2.decoder_layers.6.layernorm_before.bias', 'decoder2.decoder_layers.6.attention.attention.query.weight', 'decoder1.decoder_layers.7.output.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.weight', 'decoder1.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.6.attention.attention.query.bias', 'decoder2.decoder_layers.3.intermediate.dense.bias', 'decoder1.decoder_layers.0.attention.attention.query.bias', 'decoder2.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.0.layernorm_before.weight', 'decoder2.decoder_layers.2.intermediate.dense.weight', 'decoder2.decoder_layers.5.attention.output.dense.bias', 'decoder2.decoder_layers.0.layernorm_after.weight', 'decoder1.decoder_layers.5.attention.attention.value.weight', 'decoder1.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.4.layernorm_before.weight', 'decoder2.decoder_layers.5.output.dense.bias', 'decoder1.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.1.layernorm_before.weight', 'decoder2.decoder_layers.7.attention.output.dense.bias', 'decoder2.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.7.intermediate.dense.weight', 'decoder2.decoder_layers.5.intermediate.dense.weight', 'decoder2.decoder_layers.3.attention.attention.value.bias', 'decoder2.decoder_layers.0.intermediate.dense.weight', 'decoder1.decoder_layers.3.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_before.weight', 'decoder2.decoder_layers.2.layernorm_after.bias', 'decoder2.decoder_layers.5.attention.attention.value.weight', 'decoder2.decoder_layers.1.output.dense.weight', 'decoder1.decoder_layers.1.intermediate.dense.bias', 'decoder2.decoder_layers.7.attention.attention.key.weight', 'decoder1.decoder_layers.2.output.dense.bias', 'decoder2.decoder_layers.6.output.dense.weight', 'decoder1.decoder_layers.6.intermediate.dense.weight', 'decoder1.decoder_layers.7.output.dense.bias', 'decoder1.decoder_layers.5.layernorm_before.bias', 'decoder2.decoder_layers.1.layernorm_after.bias', 'decoder2.decoder_layers.5.layernorm_after.weight', 'decoder2.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.7.attention.attention.query.weight', 'decoder1.decoder_layers.6.attention.output.dense.weight', 'decoder1.decoder_layers.3.layernorm_after.weight', 'decoder1.decoder_layers.4.layernorm_before.bias', 'decoder1.decoder_layers.6.intermediate.dense.bias', 'decoder2.decoder_layers.2.layernorm_after.weight', 'decoder2.decoder_layers.1.attention.attention.query.weight', 'decoder1.decoder_layers.3.intermediate.dense.bias', 'decoder1.decoder_layers.4.layernorm_after.bias', 'decoder2.decoder_layers.7.attention.attention.value.bias', 'decoder1.decoder_layers.5.layernorm_before.weight', 'decoder1.decoder_layers.2.intermediate.dense.bias', 'decoder2.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.key.bias', 'decoder1.decoder_layers.4.intermediate.dense.bias', 'decoder2.decoder_layers.3.intermediate.dense.weight', 'decoder1.decoder_layers.4.attention.attention.query.bias', 'decoder1.decoder_layers.5.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_after.bias', 'decoder1.decoder_layers.3.attention.attention.query.bias', 'decoder1.decoder_layers.1.output.dense.bias', 'decoder2.decoder_layers.6.attention.attention.value.weight', 'decoder2.decoder_pos_embed', 'decoder2.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.0.layernorm_before.bias', 'decoder1.decoder_embed.bias', 'decoder1.decoder_layers.3.attention.attention.key.bias', 'decoder1.decoder_layers.1.attention.output.dense.bias', 'decoder1.decoder_layers.4.attention.output.dense.bias', 'decoder2.decoder_layers.7.output.dense.bias', 'decoder2.decoder_layers.4.attention.attention.value.bias', 'decoder2.decoder_layers.5.layernorm_before.weight', 'decoder2.decoder_layers.2.output.dense.weight', 'decoder2.decoder_layers.7.attention.attention.query.bias', 'decoder2.decoder_layers.0.output.dense.bias', 'decoder2.decoder_layers.7.layernorm_before.bias', 'decoder1.decoder_layers.5.layernorm_after.weight', 'decoder1.decoder_layers.2.attention.attention.value.weight', 'decoder2.decoder_layers.0.attention.output.dense.bias', 'decoder1.decoder_layers.0.attention.output.dense.weight', 'decoder2.decoder_layers.7.intermediate.dense.bias', 'decoder1.decoder_layers.1.layernorm_after.weight', 'decoder2.decoder_pred.weight', 'decoder1.decoder_layers.7.attention.attention.key.weight', 'decoder2.decoder_layers.4.attention.attention.query.bias', 'decoder2.decoder_layers.1.intermediate.dense.weight', 'decoder1.decoder_embed.weight', 'decoder1.decoder_layers.1.attention.attention.value.bias', 'decoder1.decoder_layers.3.attention.output.dense.bias', 'decoder2.decoder_layers.0.attention.attention.value.weight', 'decoder1.decoder_layers.0.attention.attention.key.bias', 'decoder2.decoder_layers.4.layernorm_before.weight', 'decoder1.decoder_layers.3.attention.attention.key.weight', 'decoder2.decoder_layers.4.attention.attention.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_models.CustomViT import CustomViT\n",
    "from custom_models.CustomViTMAE import CustomViTMAE\n",
    "import torch\n",
    "# call CustomViT\n",
    "from transformers import AutoImageProcessor, ViTMAEForPreTraining, ViTMAEConfig\n",
    "from PIL import Image\n",
    "\n",
    "output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "\n",
    "# Initialize a new CustomViTMAE model\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "vit_config = ViTMAEConfig.from_pretrained(model_name)\n",
    "vit_config.output_hidden_states=True\n",
    "vit_model = CustomViT.from_pretrained(model_name,config=vit_config)\n",
    "\n",
    "\n",
    "model_name = \"facebook/vit-mae-base\"\n",
    "\n",
    "config = ViTMAEConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states=True\n",
    "\n",
    "# load from pretrained model and replace the original encoder with custom encoder\n",
    "custom_model = CustomViTMAE.from_pretrained(\"facebook/vit-mae-base\",config=config)\n",
    "custom_model.vit = vit_model\n",
    "\n",
    "# Load the state_dict from the saved model\n",
    "state_dict = torch.load(f\"{output_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Apply the state_dict to the custom_model\n",
    "custom_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "custom_model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_dataloader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        pixel_values1 = batch[\"pixel_values1\"].to(device)\n",
    "        pixel_values2 = batch[\"pixel_values2\"].to(device)\n",
    "        outputs = custom_model(pixel_values, pixel_values1, pixel_values2)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mloss)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(outputs.loss)\n",
    "print(outputs.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # print(batch.keys())\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        pixel_values1 = batch[\"pixel_values1\"].to(device)\n",
    "        pixel_values2 = batch[\"pixel_values2\"].to(device)\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # bf_forward = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values,pixel_values1,pixel_values2,noise=None)\n",
    "\n",
    "        # af_forward = time.time()\n",
    "        # print(\"forward time: \", af_forward - bf_forward)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        # af_backward = time.time()\n",
    "        # print(\"backward time: \", af_backward - af_forward)\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # af_step = time.time()\n",
    "        # print(\"step time: \", af_step - af_backward)\n",
    "\n",
    "        total_loss += loss.item() * pixel_values.size(0)\n",
    "        total_samples += pixel_values.size(0)\n",
    "\n",
    "        if total_samples % (batch_size * 1) == 0: # log every 1 batches\n",
    "          wandb.log({'loss': loss.item()} )\n",
    "          print(loss.item())\n",
    "        \n",
    "        # af_loss_cal = time.time()\n",
    "        # print(\"loss cal time: \", af_loss_cal - af_step)\n",
    "\n",
    "    return total_loss / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyz/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomViTMAE(\n",
       "  (vit): CustomViT(\n",
       "    (embeddings): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (encoder): ViTMAEEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTMAELayer(\n",
       "          (attention): ViTMAEAttention(\n",
       "            (attention): ViTMAESelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTMAESelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTMAEIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTMAEOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (embeddings1): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (embeddings2): ViTMAEEmbeddings(\n",
       "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=2304, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (decoder1): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (decoder2): ViTMAEDecoder(\n",
       "    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (decoder_layers): ModuleList(\n",
       "      (0-7): 8 x ViTMAELayer(\n",
       "        (attention): ViTMAEAttention(\n",
       "          (attention): ViTMAESelfAttention(\n",
       "            (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTMAESelfOutput(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTMAEIntermediate(\n",
       "          (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTMAEOutput(\n",
       "          (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.optimization import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 1\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 0\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "optimizer = AdamW(custom_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "custom_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tyz/Desktop/11_777/camelmera/models/gym/multimodal/wandb/run-20230423_220149-ds185kqv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yadix/11777/runs/ds185kqv' target=\"_blank\">AmericanDinerExposure20230423-220148</a></strong> to <a href='https://wandb.ai/yadix/11777' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yadix/11777' target=\"_blank\">https://wandb.ai/yadix/11777</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yadix/11777/runs/ds185kqv' target=\"_blank\">https://wandb.ai/yadix/11777/runs/ds185kqv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m11777\u001b[39m\u001b[39m\"\u001b[39m,name\u001b[39m=\u001b[39menvironment_name\u001b[39m+\u001b[39mtime\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[39m=\u001b[39m train(custom_model, train_dataloader, optimizer, scheduler, device)\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/tyz/Desktop/11_777/camelmera/weights\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      8\u001b[0m     \u001b[39m# print(batch.keys())\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     pixel_values \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m     pixel_values1 \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values1\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1296\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1134\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"11777\",name=environment_name+time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(custom_model, train_dataloader, optimizer, scheduler, device)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {train_loss:.4f}\")\n",
    "    output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "    custom_model.save_pretrained(output_dir)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir='/home/tyz/Desktop/11_777/camelmera/weights'\n",
    "custom_model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
